Lack of model transparency risk for AI
Last updated: Dec 12, 2024
Non-technical risks
Governance
Traditional AI risk
Description
Lack of model transparency is due to insufficient documentation of the model design, development, and evaluation process and the absence of insights into the inner workings of the model.

Why is lack of model transparency a concern for foundation models?
Transparency is important for legal compliance, AI ethics, and guiding appropriate use of models. Missing information might make it more difficult to evaluate risks, change the model, or reuse it.  Knowledge about who built a model can also be an important factor in deciding whether to trust it. Additionally, transparency regarding how the model’s risks were determined, evaluated, and mitigated also play a role in determining model risks, identifying model suitability, and governing model usage.

Background image for risks associated with input
Example
Data and Model Metadata Disclosure

OpenAI‘s technical report is an example of the dichotomy around disclosing data and model metadata. While many model developers see value in enabling transparency for consumers, disclosure poses real safety issues and might increase the ability to misuse the models. In the GPT-4 technical report, the authors state, “Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, data set construction, training method, or similar.”

Sources:

OpenAI, March 2023

Parent topic: AI risk atlas

We provide examples covered by the press to help explain many of the foundation models' risks. Many of these events covered by the press are either still evolving or have been resolved, and referencing them can help the reader understand the potential risks and work towards mitigations. Highlighting these examples are for illustrative purposes only.
